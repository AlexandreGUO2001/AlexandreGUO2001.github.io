---
---

@article{guo2025complexity,
  title={Complexity Analysis of Normalizing Constant Estimation: from Jarzynski Equality to Annealed Importance Sampling and beyond},
  author={Guo, Wei and Tao, Molei and Chen, Yongxin},
  journal={arXiv preprint arXiv:2502.04575},
  year={2025},
  html={https://arxiv.org/abs/2502.04575},
  abstract={Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity.}
}

@article{ren2025fast,
  title={Fast solvers for discrete diffusion models: Theory and applications of high-order algorithms},
  author={Ren, Yinuo and Chen, Haoxuan and Zhu, Yuchen and Guo, Wei and Chen, Yongxin and Rotskoff, Grant M and Tao, Molei and Ying, Lexing},
  journal={arXiv preprint arXiv:2502.00234},
  year={2025},
  html={https://arxiv.org/abs/2502.00234},
  abstract={Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis. However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms. Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $\tau$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $\tau$-leaping is limited by its first-order accuracy. In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error. We rigorously analyze the proposed schemes and establish the second-order accuracy of the $\theta$-trapezoidal method in KL divergence. Empirical evaluations on GPT-2 level text and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints.}
}


@article{guo2024plug,
  title={Plug-and-Play Controllable Generation for Discrete Masked Models},
  author={Guo, Wei and Zhu, Yuchen and Tao, Molei and Chen, Yongxin},
  journal={arXiv preprint arXiv:2410.02143},
  html = {https://arxiv.org/abs/2410.02143},
  year={2024},
  abstract={This article makes discrete masked models for the generative modeling of discrete data controllable. The goal is to generate samples of a discrete random variable that adheres to a posterior distribution, satisfies specific constraints, or optimizes a reward function. This methodological development enables broad applications across downstream tasks such as class-specific image generation and protein design. Existing approaches for controllable generation of masked models typically rely on task-specific fine-tuning or additional modifications, which can be inefficient and resource-intensive. To overcome these limitations, we propose a novel plug-and-play framework based on importance sampling that bypasses the need for training a conditional score. Our framework is agnostic to the choice of control criteria, requires no gradient information, and is well-suited for tasks such as posterior sampling, Bayesian inverse problems, and constrained generation. We demonstrate the effectiveness of our approach through extensive experiments, showcasing its versatility across multiple domains, including protein design.}
}

@inproceedings{guo2025provable,
  title={Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling},
  author={Guo, Wei and Tao, Molei and Chen, Yongxin},
  booktitle={The Thirteenth International Conference on Learning Representations},
  html = {https://openreview.net/forum?id=P6IVIoGRRg},
  year={2025},
  abstract={We consider the outstanding problem of sampling from an unnormalized density that may be non-log-concave and multimodal. To enhance the performance of simple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type have been widely used. However, quantitative theoretical guarantees of these techniques are under-explored. This study takes a first step toward providing a non-asymptotic analysis of annealed MCMC. Specifically, we establish, for the first time, an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\cal A}^2}{\varepsilon^6}\right)$ for the simple annealed Langevin Monte Carlo algorithm to achieve $\varepsilon^2$ accuracy in Kullback-Leibler divergence to the target distribution $\pi\propto{\rm e}^{-V}$ on $\mathbb{R}^d$ with $\beta$-smooth potential $V$. Here, ${\cal A}$ represents the action of a curve of probability measures interpolating the target distribution $\pi$ and a readily sampleable distribution.}
}

@thesis{guo2023theoretical,
  title={[Bachelor's Thesis] Theoretical Analysis of the Approximation Properties of Score-Based Generative Models},
  author={Wei Guo},
  year={2023},
  type={Bachelor's Thesis},
  pdf={https://alexandreguo2001.github.io/assets/pdf/Wei_Guo_Undergrad_Thesis.pdf},
  abstract={Score-based generative models leverage a neural network to approximate the score function of the data distribution and employ stochastic or ordinary differential equations to sample from the learned model, which have achieved state-of-the-art performance in tasks such as text-to-image generation and audio synthesis. To elucidate and demystify their empirical success, we investigate their approximation properties in this paper, and focus on two main algorithms: the inexact Langevin Monte Carlo (LMC), and the diffusion models. We establish convergence guarantees of inexact LMC in various metrics (e.g., total-variational distance, Wasserstein-2 distance, and RÃ©nyi divergence) under different assumptions of accuracy in score estimation (namely, accuracy in the sense of $L^\infty$, $L^2$, and moment generating function). We also provide a comprehensive review of different approaches to analyze the approximation properties of diffusion models, including the variational approach, the Fokker-Planck approach, the Girsanov approach, the $L^\infty\to L^2$ approach, the restoration-degradation approach, and the KL divergence decomposition approach. Our analysis reveals that under mild assumptions of the target distribution and the discretization scheme, score-based generative models can arbitrarily approximate the target distribution provided that the score estimate is sufficiently precise, which partially sheds light on the theoretical foundations of score-based generative models.}
}
